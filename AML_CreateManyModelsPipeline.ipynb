{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04efb8fb",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - Many Models + MLflow\n",
    "\n",
    "Sample machine learning pipeline for training/evaluating/registering multiple regression models (Scikit-Learn `ElasticNetCV`) using MLflow. This pipeline was adapted from Microsoft's [Many Models Solution Accelerator](https://github.com/microsoft/solution-accelerator-many-models) and utilizes [`ParallelRunStep`](https://learn.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py)s throughout to achieve parallelism on training and evaluation - for large scale training efforts this can afford substantial performance benefits. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3435d61",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc8096-4694-4a79-801c-f00e7db341b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd4797c",
   "metadata": {},
   "source": [
    "### Connect to AML workspace, provision compute cluster, and gather reference to default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82510f-2074-46b6-9721-360df9213494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = None\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "except Exception:\n",
    "    ws = Workspace(subscription_id=os.getenv('SUBSCRIPTION_ID'),  resource_group = os.getenv('RESOURCE_GROUP'), workspace_name = os.getenv('WORKSPACE_NAME'))\n",
    "\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cluster001'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=3)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2994ed2",
   "metadata": {},
   "source": [
    "### Create pipeline execution environment\n",
    "Environment here is defined from the `requirements.txt` file located in this repository. This environment is registered in the AML workspace following registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e4bc5-66e6-42af-a2d6-26c149aa9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_env = Environment.from_pip_requirements(name='ManyModelsEnv', file_path='./requirements.txt')\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = aml_env\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.docker.arguments = ['-v', '/var/run/docker.sock:/var/run/docker.sock']\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\n",
    "\n",
    "#Register environment for reuse \n",
    "run_config.environment.register(ws)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32aa869f",
   "metadata": {},
   "source": [
    "### Define output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51f763-fb5d-4c4b-a464-ef3a487dda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = OutputFileDatasetConfig(name='HomePrices_Raw_Data', destination=(default_ds, 'homeprices_raw_data/{run-id}')).read_delimited_files().register_on_complete(name='HomePrices_Raw_Data')\n",
    "training_data = OutputFileDatasetConfig(name='HomePrices_Training_Data', destination=(default_ds, 'homeprices_training_data/{run-id}')).register_on_complete(name='HomePrices_Training_Data')\n",
    "testing_data = OutputFileDatasetConfig(name='HomePrices_Testing_Data', destination=(default_ds, 'homeprices_testing_data/{run-id}')).register_on_complete(name='HomePrices_Testing_Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de72c258",
   "metadata": {},
   "source": [
    "### Define location for training/evaluation parallel run outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ddadb-bc49-4387-8290-9826a98bfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"training_output\", datastore=default_ds)\n",
    "test_output_dir = PipelineData(name=\"testing_output\", datastore=default_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd4f3fd7",
   "metadata": {},
   "source": [
    "### Define pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbcd43-bcc4-4b58-9ae8-8d4f2b95c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = PipelineParameter(name='testing_size', default_value=0.3)\n",
    "target_column = PipelineParameter(name='target_column', default_value='target')\n",
    "deployment_name = PipelineParameter(name='deployment_name', default_value='manymodelsdeployment')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c984ca4",
   "metadata": {},
   "source": [
    "### Define pipeline execution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680c941-bac2-4de0-a2db-d616569f07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data from AML-linked datastore\n",
    "# Register tabular dataset after retrieval\n",
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data',\n",
    "    script_name='get_data.py',\n",
    "    arguments =['--raw_data', raw_data],\n",
    "    outputs=[raw_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "# Load raw data and split into test and train\n",
    "# subsets according to the specified split percentage\n",
    "# and logical categories \n",
    "split_data_step = PythonScriptStep(\n",
    "    name='Split Train and Test Data',\n",
    "    script_name='split_data.py',\n",
    "    arguments =['--training_data', training_data,\n",
    "                '--testing_data', testing_data,\n",
    "                '--testing_size', testing_size],\n",
    "    inputs=[raw_data.as_input(name='Raw_Data')],\n",
    "    outputs=[training_data, testing_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "# For each training sub-dataset, train a single model. \n",
    "# Use a parallel run step here to train multiple models\n",
    "# simultaneously.\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "# Define the ParallelRunConfig object\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    entry_script='train_models.py', # the user script that will be run in parallel\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    environment=aml_env,\n",
    "    compute_target=cpu_cluster,\n",
    "    node_count=1\n",
    ")\n",
    "\n",
    "train_models_step = ParallelRunStep(\n",
    "    name='Train Models',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[training_data.as_input(name='train_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "# Prior to model registration, evaluate each\n",
    "# newly trained model against previous variants. \n",
    "# Register the best performer in all cases.\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    entry_script='evaluate_models.py', # the user script that will be run in parallel\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    environment=aml_env,\n",
    "    compute_target=cpu_cluster,\n",
    "    node_count=1\n",
    ")\n",
    "\n",
    "evaluate_models_step = ParallelRunStep(\n",
    "    name='Evaluate Models',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[testing_data.as_input(name='test_data')],\n",
    "    output=test_output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "evaluate_models_step.run_after(train_models_step)\n",
    "\n",
    "# Finally, create a containerized deployment which contains\n",
    "# all trained models (best version of each)\n",
    "create_deployment_step = PythonScriptStep(\n",
    "    name='Package Models',\n",
    "    script_name='package_models.py',\n",
    "    arguments =['--deployment_name', deployment_name],\n",
    "    inputs=[training_data.as_input(name='train_data').as_download('./training-data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "create_deployment_step.run_after(evaluate_models_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba7be47",
   "metadata": {},
   "source": [
    "### Assemble steps into an executable pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fba26c-104a-40b8-b2c1-825785b4c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_data_step, split_data_step, train_models_step, evaluate_models_step, create_deployment_step])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d36d0a1",
   "metadata": {},
   "source": [
    "### Submit pipeline run as new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04be510-4e85-4f97-9dd4-5049537cc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = os.getenv('EXPERIMENT_NAME', 'many-models-pipeline-run')\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2e0b197ae37a172a070d322ccdfd2dc89f3ea78020965c5820be3c5f3a0dbfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
